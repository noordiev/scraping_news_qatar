{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import csv\n",
    "import nltk\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "nltk.download('punkt')\n",
    "\n",
    "BASE_URL = \"https://thepeninsulaqatar.com\"\n",
    "CATEGORIES = [\n",
    "              \"https://thepeninsulaqatar.com/category/cinema\",\n",
    "              \"https://thepeninsulaqatar.com/category/music\",\n",
    "              \"https://thepeninsulaqatar.com/category/general\",\n",
    "              \"https://thepeninsulaqatar.com/category/Gulf\"]\n",
    "\n",
    "def requests_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504)):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def safe_get(url, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return requests_retry_session().get(url)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Connection error on {url}, retrying... ({i + 1}/{retries})\")\n",
    "            time.sleep(5)  # Add sleep between retries\n",
    "    print(f\"Failed to get {url} after {retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def get_article_links(page_url):\n",
    "    response = safe_get(page_url)\n",
    "    if response is None:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('div.col-sm-6.item')\n",
    "    article_links = [BASE_URL + article.find('a', class_='title')['href'] for article in articles if article.find('a', class_='title')]\n",
    "    return article_links\n",
    "\n",
    "def scrape_article(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "\n",
    "        data = {\n",
    "            'title': article.title,\n",
    "            'authors': article.authors,\n",
    "            'publish_date': article.publish_date,\n",
    "            'text': article.text,\n",
    "            'keywords': article.keywords,\n",
    "            'summary': article.summary,\n",
    "            'url': url\n",
    "        }\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_next_page_url(soup):\n",
    "    active_page = soup.select_one('li.page-item a.page-link.active')\n",
    "    if not active_page:\n",
    "        return None\n",
    "\n",
    "    active_page_li = active_page.find_parent('li')\n",
    "    if not active_page_li:\n",
    "        return None\n",
    "\n",
    "    next_page_li = active_page_li.find_next_sibling('li')\n",
    "    if not next_page_li:\n",
    "        return None\n",
    "\n",
    "    next_page = next_page_li.find('a', class_='page-link')\n",
    "    if next_page and 'href' in next_page.attrs:\n",
    "        return next_page['href']\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_category(category_url, category_name):\n",
    "    current_url = category_url\n",
    "    category_articles = []\n",
    "\n",
    "    while current_url:\n",
    "        print(f\"Scraping page: {current_url}\")\n",
    "        \n",
    "        article_links = get_article_links(current_url)\n",
    "        if not article_links:\n",
    "            print(f\"No articles found on {current_url}\")\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            article_data = scrape_article(link)\n",
    "            if article_data:\n",
    "                category_articles.append(article_data)\n",
    "\n",
    "        response = safe_get(current_url)\n",
    "        if response is None:\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        next_page_url = get_next_page_url(soup)\n",
    "        current_url = next_page_url if next_page_url else None\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if category_articles:\n",
    "        save_to_csv(f'scraped_articles_{category_name}.csv', category_articles)\n",
    "        print(f\"Saved articles for category: {category_name}\")\n",
    "        category_articles.clear()\n",
    "\n",
    "def save_to_csv(filename, data):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def main():\n",
    "    for category_url in CATEGORIES:\n",
    "        category_name = category_url.split('/')[-1] \n",
    "        print(f\"Scraping category: {category_name}\")\n",
    "        scrape_category(category_url, category_name)\n",
    "        print(f\"Finished scraping category: {category_name}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
